{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Taxi-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Decode State', [1, 4, 3, 2])\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_state = env.reset()\n",
    "# (taxi row, taxi column, passenger index, destination index)\n",
    "print (\"Decode State\", list(env.env.decode(initial_state)))\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ideallly, the size od Q-table is state_space_size x action_space_size\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_update(q_table, env, state, epsilon):\n",
    "    \"\"\"\"\n",
    "    Update the Q-values according to the Q-learning equation.\n",
    "    \"\"\"\n",
    "    if random.uniform(0,1) > epsilon:\n",
    "        action = env.action_space.sample() # select a random action\n",
    "    else:\n",
    "        action = select_optimal_action(q_table, state) # select an optimal action\n",
    "        \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    old_q_value = q_table[state][action]\n",
    "    \n",
    "    # Maximum q_value for the action in next state\n",
    "    next_max = np.max(q_table[next_state])\n",
    "    \n",
    "    new_q_value = (1 - alpha) * old_q_value + alpha * (reward + gamma * next_max)\n",
    "    \n",
    "    # Update te q_value\n",
    "    q_table[state][action] = new_q_value\n",
    "    \n",
    "    return next_state, reward, done\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def select_optimal_action(q_table, state):\n",
    "    \"\"\"\n",
    "    Given a state, select the action from the action  space having the\n",
    "    highest Q-value in the q_table.\n",
    "    \"\"\"\n",
    "    if np.sum(q_table[state]) == 0:\n",
    "        return random.randint(0, q_table.shape[1]-1)\n",
    "    return np.argmax(q_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(q_table, env, num_episodes, epsilon):\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        epochs = 0\n",
    "        num_penalties, total_reward = 0, 0\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state, reward, done = q_learning_update(q_table, env, state, epsilon)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials(q_table, env, num_trials):\n",
    "    data_by_episode = []\n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        state = env.reset()\n",
    "        epochs, num_penalties, episode_reward = 0,0,0\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            next_action = select_optimal_action(q_table, state)\n",
    "            state, reward, done, _ = env.step(next_action)\n",
    "            \n",
    "            if reward == -10:\n",
    "                num_penalties += 1\n",
    "                \n",
    "            epochs += 1\n",
    "            episode_reward += reward\n",
    "            \n",
    "        data_by_episode.append((epochs, num_penalties, episode_reward))\n",
    "        \n",
    "    return data_by_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(trial_data):\n",
    "    epochs, penalties, rewards = zip(*trial_data)\n",
    "    total_epochs, total_penalties, total_reward = sum(epochs), sum(penalties), sum(rewards)\n",
    "    num_trials = len(epochs)\n",
    "    average_time = total_epochs / float(num_trials)\n",
    "    average_penalties = total_penalties / float(num_trials)\n",
    "    average_reward_per_move = total_reward/float(total_epochs)\n",
    "    \n",
    "    return (average_time, average_penalties, average_reward_per_move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time steps taken: 11.64\n",
      "Average number of penalties incrred: 0.0\n",
      "Average reward per move: 0.80412371134\n"
     ]
    }
   ],
   "source": [
    "data_to_plot = []\n",
    "train_episodes_before_eval = 100\n",
    "num_episodes_for_eval = 50\n",
    "\n",
    "while True:\n",
    "    q_table = train_agent(q_table, env, train_episodes_before_eval, epsilon=epsilon)\n",
    "    trial_data = run_trials(q_table, env, num_episodes_for_eval)\n",
    "    \n",
    "    average_time, average_penalties, average_reward_per_move = calculate_stats(trial_data)\n",
    "    \n",
    "    data_to_plot.append((average_time, average_penalties, average_reward_per_move))\n",
    "    \n",
    "    if average_penalties == 0 and average_reward_per_move > 0.8:\n",
    "        print(\"Average time steps taken: {}\".format(average_time))\n",
    "        print(\"Average number of penalties incrred: {}\".format(average_penalties))\n",
    "        print(\"Average reward per move: {}\".format(average_reward_per_move))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Requirement already up-to-date: matplotlib in /Users/hasib/Library/Python/2.7/lib/python/site-packages (2.2.3)\n",
      "Requirement already up-to-date: python in /System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload (2.7.10)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /Users/hasib/Library/Python/2.7/lib/python/site-packages (from matplotlib) (2.7.5)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /Users/hasib/Library/Python/2.7/lib/python/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: subprocess32 in /Users/hasib/Library/Python/2.7/lib/python/site-packages (from matplotlib) (3.5.3)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from matplotlib) (2013.7)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10 in /Library/Python/2.7/site-packages/six-1.11.0-py2.7.egg (from matplotlib) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: backports.functools-lru-cache in /Users/hasib/Library/Python/2.7/lib/python/site-packages (from matplotlib) (1.5)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from matplotlib) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7.1 in /Library/Python/2.7/site-packages (from matplotlib) (1.14.2)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /Users/hasib/Library/Python/2.7/lib/python/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /Users/hasib/Library/Python/2.7/lib/python/site-packages (from kiwisolver>=1.0.1->matplotlib) (40.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U matplotlib --user python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 100,\n",
       " 200,\n",
       " 300,\n",
       " 400,\n",
       " 500,\n",
       " 600,\n",
       " 700,\n",
       " 800,\n",
       " 900,\n",
       " 1000,\n",
       " 1100,\n",
       " 1200,\n",
       " 1300,\n",
       " 1400,\n",
       " 1500,\n",
       " 1600,\n",
       " 1700,\n",
       " 1800,\n",
       " 1900,\n",
       " 2000,\n",
       " 2100,\n",
       " 2200,\n",
       " 2300,\n",
       " 2400,\n",
       " 2500,\n",
       " 2600,\n",
       " 2700,\n",
       " 2800,\n",
       " 2900,\n",
       " 3000,\n",
       " 3100,\n",
       " 3200,\n",
       " 3300,\n",
       " 3400,\n",
       " 3500,\n",
       " 3600,\n",
       " 3700,\n",
       " 3800,\n",
       " 3900,\n",
       " 4000,\n",
       " 4100,\n",
       " 4200,\n",
       " 4300,\n",
       " 4400,\n",
       " 4500,\n",
       " 4600,\n",
       " 4700,\n",
       " 4800,\n",
       " 4900,\n",
       " 5000,\n",
       " 5100,\n",
       " 5200,\n",
       " 5300,\n",
       " 5400,\n",
       " 5500,\n",
       " 5600,\n",
       " 5700,\n",
       " 5800,\n",
       " 5900,\n",
       " 6000,\n",
       " 6100,\n",
       " 6200,\n",
       " 6300,\n",
       " 6400,\n",
       " 6500,\n",
       " 6600,\n",
       " 6700,\n",
       " 6800,\n",
       " 6900,\n",
       " 7000,\n",
       " 7100,\n",
       " 7200,\n",
       " 7300,\n",
       " 7400,\n",
       " 7500,\n",
       " 7600,\n",
       " 7700,\n",
       " 7800,\n",
       " 7900,\n",
       " 8000,\n",
       " 8100,\n",
       " 8200,\n",
       " 8300,\n",
       " 8400,\n",
       " 8500,\n",
       " 8600,\n",
       " 8700,\n",
       " 8800,\n",
       " 8900,\n",
       " 9000,\n",
       " 9100,\n",
       " 9200,\n",
       " 9300,\n",
       " 9400,\n",
       " 9500,\n",
       " 9600,\n",
       " 9700,\n",
       " 9800,\n",
       " 9900,\n",
       " 10000,\n",
       " 10100,\n",
       " 10200,\n",
       " 10300,\n",
       " 10400,\n",
       " 10500,\n",
       " 10600,\n",
       " 10700,\n",
       " 10800,\n",
       " 10900,\n",
       " 11000,\n",
       " 11100,\n",
       " 11200,\n",
       " 11300,\n",
       " 11400,\n",
       " 11500,\n",
       " 11600,\n",
       " 11700,\n",
       " 11800,\n",
       " 11900,\n",
       " 12000,\n",
       " 12100,\n",
       " 12200,\n",
       " 12300,\n",
       " 12400,\n",
       " 12500,\n",
       " 12600,\n",
       " 12700,\n",
       " 12800,\n",
       " 12900,\n",
       " 13000,\n",
       " 13100,\n",
       " 13200,\n",
       " 13300,\n",
       " 13400,\n",
       " 13500,\n",
       " 13600,\n",
       " 13700,\n",
       " 13800,\n",
       " 13900,\n",
       " 14000,\n",
       " 14100,\n",
       " 14200,\n",
       " 14300,\n",
       " 14400,\n",
       " 14500,\n",
       " 14600,\n",
       " 14700,\n",
       " 14800,\n",
       " 14900,\n",
       " 15000,\n",
       " 15100,\n",
       " 15200,\n",
       " 15300,\n",
       " 15400,\n",
       " 15500,\n",
       " 15600,\n",
       " 15700,\n",
       " 15800,\n",
       " 15900,\n",
       " 16000,\n",
       " 16100,\n",
       " 16200,\n",
       " 16300,\n",
       " 16400,\n",
       " 16500,\n",
       " 16600,\n",
       " 16700,\n",
       " 16800,\n",
       " 16900,\n",
       " 17000,\n",
       " 17100,\n",
       " 17200,\n",
       " 17300,\n",
       " 17400,\n",
       " 17500,\n",
       " 17600,\n",
       " 17700,\n",
       " 17800,\n",
       " 17900,\n",
       " 18000,\n",
       " 18100,\n",
       " 18200,\n",
       " 18300,\n",
       " 18400,\n",
       " 18500,\n",
       " 18600,\n",
       " 18700,\n",
       " 18800,\n",
       " 18900,\n",
       " 19000,\n",
       " 19100,\n",
       " 19200,\n",
       " 19300,\n",
       " 19400,\n",
       " 19500,\n",
       " 19600,\n",
       " 19700,\n",
       " 19800,\n",
       " 19900,\n",
       " 20000,\n",
       " 20100,\n",
       " 20200,\n",
       " 20300,\n",
       " 20400,\n",
       " 20500,\n",
       " 20600,\n",
       " 20700,\n",
       " 20800,\n",
       " 20900,\n",
       " 21000,\n",
       " 21100,\n",
       " 21200]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "time_steps_by_evals = [data[0] for data in data_to_plot]\n",
    "penalties_by_evals = [data[1] for data in data_to_plot]\n",
    "rewards_by_evals = [data[2] for data in data_to_plot]\n",
    "\n",
    "episode_counts = [i*train_episodes_before_eval for i in range(len(data_to_plot))]\n",
    "episode_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10478dd90>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'to_rgba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/Users/hasib/Library/Python/2.7/lib/python/site-packages/ipykernel/pylab/backend_inline.pyc\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     37\u001b[0m             display(\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             )\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hasib/Library/Python/2.7/lib/python/site-packages/ipykernel/pylab/backend_inline.pyc\u001b[0m in \u001b[0;36m_fetch_figure_metadata\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;34m\"\"\"Get some metadata to help with displaying a figure.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# determine if a background is needed for legibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m# the background is transparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         ticksLight = _is_light([label.get_color()\n",
      "\u001b[0;32m/Users/hasib/Library/Python/2.7/lib/python/site-packages/ipykernel/pylab/backend_inline.pyc\u001b[0m in \u001b[0;36m_is_transparent\u001b[0;34m(color)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;34m\"\"\"Determine transparency from alpha.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'to_rgba'"
     ]
    }
   ],
   "source": [
    "plt.plot(episode_counts, time_steps_by_evals)\n",
    "plt.xlabel(\"Number of Episodes\")\n",
    "plt.ylabel(\"Time step to finish episode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = train_agent(q_table, env, 50000, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.42914074,  -2.30953155,  -2.30968412,  -2.3851712 ,\n",
       "       -11.36610372, -11.32836826])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_state = env.reset()\n",
    "print(initial_state)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.18255258,  -2.30821127,  -2.38468666,  -2.18376611,\n",
       "       -11.25275858, -11.21689027])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
